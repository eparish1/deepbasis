%\documentclass{article}
\documentclass[3p,computermodern,10pt]{elsarticle}
\input{packages}
\input{commands}
\begin{document}
\begin{frontmatter}

\title{Parametric space--time model reduction with deep bases}
%\title{The windowed least-squares framework for model reduction of dynamical systems}

%\author[a]{Eric J. Parish and Kevin T. Carlberg}
%\ead{ejparis@sandia.gov}
\begin{abstract}
\end{abstract}
\end{frontmatter}


%\maketitle
\section{Introduction}
The simulation of parameterized partial differential equations is ubiquitous in computational science, playing important roles in uncertainty quantification, design and optimization, and control, to name a few areas. Despite the tremendous growth in computing capabilities in the past several decades, however, simulating partial differential equations for a single parameter instance often remains a computationally intensive process. For many-query analyses, such as uncertainty quantification and design, this high computational cost becomes a bottleneck. As a result, analysts often rely on low-cost computational models to generate approximate solutions. 

A wide variety of techniques exist for computing low-cost approximate solutions, including projection-based reduced-order models (ROMs), coarse mesh and/or reduced physics solutions, kriging, and machine learning regression models. Projection-based reduced-order models comprise an approximation strategy that has received significant attention. These techniques operate in an offline--online paradigm. First, in the offline stage, a computationally expensive training process is undertaken to compute a low-dimensional \textit{trial subspace} on which the system's state can be well approximated (e.g., via the proper orthogonal decomposition). Second, projection-based reduced-order models execute an inexpensive \textit{online} stage in where they compute approximate solutions to the system that reside on this low-dimensional trial subspace via, e.g., Galerkin projection. 

Projection-based reduced-order modeling technologies are mature for both linear and nonlinear systems. Techniques have been developed that, e.g., yield space--time optimality~\cite{choi_stlspg,constantine_strom,yuki_stlspg,parish_wls,townePOD}, preserve physical structure (e.g., conservation), adapt the trial subspace, account for observability, controlability, and $\mathcal{H}^2$ optimality, address the closure problem. This rich body of work has enabled the construction of accurate and efficient ROMs for a wide class of physical systems. One class of systems that remain challenging for ROMs, however, are those whose solutions exhibit a slowly decaying \textit{Kolmogorov n-width}; the Kolmogorov $n$-width provides a metric for quantifying the optimal linear subspace. As ROMs traditionally restrict the state to belong to a \textit{linear} trial subspace, they are known to perform poorly for this class of problems. Approaches that have been proposed to address this issue include transporting and morphing the trial-subspace or space--time mesh, developing local subspaces that are tailored to a particular region of a space--time--parameter domain, adapting and/or refining the basis, and employing a \textit{nonlinear} trial manifold~\cite{LeeCarlberg,kim2020fast}. This latter approach is, at the time of this writing, of particular interest. These manifold approaches operate by restricting the state to live on a low-dimensional (nonlinear) \textit{manifold} (identified, for example, through deep convolutional autoencoders~\cite{LeeCarlberg}). By employing a nonlinear manifold, these approaches are able to overcome the fundamental barrier posed by the Kolmogorov $n$-width. Numerical experiments have demonstrated that, for a given latent dimension, a manifold-based approximation can yield far more accurate solutions than a linear subspace for transport problems. While promising, nonlinear manifold ROMs face several significant challenges, including offline training cost, online model execution cost, generalization, and scalability. 

It is well known that nonlinear phenomena can become linear in higher-dimensional spaces; this is the thesis, for example, behind support vector machines and kernel principal component analysis. Constructing subspaces for projection-based reduced-order models is no different. In all the approaches mentioned above, save for Refs.~\cite{}, parameter-independent trial subspaces/manifolds are constructed for parameterized systems; e.g., one trial subspace/manifold is constructed for the system of interest, and this subspace/manifold is held constant across the entire parameter domain. While this perspective is justified for certain classes of systems, e.g., fluid flow with recurrent coherent structures appearing in similar spatial locations, it is non-intuitive for a wide variety of problems, e.g., fluid flow with parameterized shocks and advecting fronts.  Indeed, it is from this parametric-constant perspective that a slowly decaying Kolmogorov n-width is observed for this latter class of problems, and nonlinear dimension reduction techniques are proposed as the solution.

In this work, we propose that the Kolmogorov $n$-width limitation can be overcome by a change in perspective. Specifically, we propose a ROM framework in where the solution to a parameterized dynamical system is restricted to belong to a low-dimensional \textit{parametric--space--time} trial subspace, and a ROM is constructed via, e.g., Galerkin projection.  One solution to this reduced-order model then yields the system response over the entire parametric--space--time domain. To construct these trial subspaces, we employ the same snapshot data used in traditional POD/RB-based methods. By adapting this higher-dimensional perspective, we show that accurate solutions can be obtained with relatively few basis vectors for problems that are traditionally characterized as being difficult due to the Kolmogorov $n$-width. 

The first ingredient in our proposed methodology is the construction of a parametric--space--time trial subspace. To the best of the authors' knowledge, the only explored techniques to construct such trial subspace are Refs.~\cite{}, where local spatial subspaces are stitched together to build a monolithic subspace that is piecewise-constant in the parametric-domain. In the online phase, the solution is restricted to a particular local subspace according to the sub-region of the solution space where the high-dimensional solution lives, and a ROM is derived, e.g., via least-squares residual minimization. While this approach enables parametric-dependent subspaces, it is restricted in that (1) only piecewise-constant subspaces are considered, (2) identifying, \textit{a priori}, the number of local reduced-bases is computationally expensive, and (3) only spatial projection processes are considered, and thus the parametric dimension of the problem is not reduced. 
%An additional challenge in constructing a parametric--space--time trial subspace is the curse of dimensionality. For instance, a transient system with one spatial dimension and two parameters requires discretization of a four-dimensional domain; this process can become infeasible for systems with a large dimensional set of parameters.   

In this work, we propose a deep-learning-based mesh-free method for constructing parametric--space--time trial subspaces. In this approach, we employ deep feed forward neural networks to directly learn trial subspaces from snapshot data. This is achieved by constructing a network whose inputs comprise a parametric--space--time coordinate, and whose response is a set of basis functions evaluated at the input coordinate; a linear combination of these basis functions via the \textit{generalized coordinates} can then be used to approximate the solution over the entire parametric--space--time domain. We train this network by minimizing, e.g., the mean-squared-error between the snapshot data and the snapshot data projected onto the trial subspace; this minimization process involves jointly optimizing the network weights along with the generalized coordinates.

The second ingredient in our approach is a parametric--space--time projection process of the full-order model. We outline several such projection schemes, including monolithic Galerkin and monolithic least-squares projections, as well as local parametric-collocation projections.  
  
\begin{enumerate}
\item PROMs
\begin{itemize}
\item Project onto low-dimensional space-(time) subspace, predict for new parameter instances
\item Kolmogorov n-width
\end{itemize}
\item PINNs
\begin{itemize}
\item Learn a mapping via deep learning, minimize composite loss
\item Full-order model solve
\item Final layer is a basis
\end{itemize}
\end{enumerate}  
\section{Mathematical setting and trial subspaces}
We consider model reduction of the partial differential equation given by
\begin{equation}\label{eq:fom}
\frac{\partial \PDEState}{\partial t}(\PDEStateArgs) = \PDEFlux(\PDEState(\cdot,t,\params),\PDEStateArgs), 
\end{equation}
where $\PDEState: \PDEStateDomain \rightarrow \PDEStateCodomain$ with $\PDEState(\cdot,t,\params) \in \PDEStateSpatialTrialSpace$ is the state, $\PDEStateSpatialTrialSpace$ is an appropriate function space for the PDE of interest (e.g., $H_1(\Omega)$), $\PDEFlux(\cdot,\cdot,\cdot,\cdot) \in \RR{\NVars}$ is the right hand side (differential) operator, $\params \in \paramDomain \subset \RR{\numParams}$  are system parameters, and $\x \in \SpatialDomain$ are the spatial coordinates.

\begin{comment}
In model reduction, it is common to generate reduced-order models not for the continuous PDE~\eqref{eq:fom}, but rather for a semi-discrete form obtained after spatial discretization. To this end, we introduce a discretization of $\Omega$ into $\NSpace$ degrees of freedom characterized by the nodal points $\xd_i$, which in three-dimensions, for example, is given by the coordinates $\xd_i = (\xs_i,\ys_i,\zs_i)$. Spatial discrertization of the FOM PDE~\eqref{eq:fom} yields the semi-discrete system  
\begin{equation}\label{eq:fom_ode}
\frac{d \SemiDiscreteState}{dt}(\SemiDiscreteStateArgs) = \velocity(\SemiDiscreteState(\SemiDiscreteStateArgs),t,\params),
\end{equation}
where $\SemiDiscreteState(t,\params) \in \RR{\FomDim}$ is the semi-discrete state, $\FomDim = \NVars \NSpace$ is the total number of discrete degrees of freedom, and $\velocity: \RR{\FomDim} \times [0,T] \times \paramDomain \rightarrow \RR{\FomDim}$ is the velocity function.

Numerically solving the FOM ODE~\eqref{eq:fom_ode} requires temporal discretization. We consider linear multistep methods for this purpose. 
To this end, we introduce a uniform time
grid with time step $\Delta t$ and time instances
$t^n = n\Delta
t$, $n=0,\ldots,\numTimeSteps$. 
Applying a linear multistep method to discretize the FOM ODE \eqref{eq:FOM}
with this time grid
yields the FOM O$\Delta$E, which computes the sequence of discrete
solutions
$\FullyDiscreteState^n(\FullyDiscreteStateArgs) \approx \SemiDiscreteState(t^n,\params)$, $n=1,\ldots,\numTimeSteps$
as the implicit solution to the system of algebraic equations
\begin{equation}\label{eq:lms}
\residLMS^n
	(\FullyDiscreteState^n(\FullyDiscreteStateArgs);,\FullyDiscreteState^{n-1}(\FullyDiscreteStateArgs),\ldots,\FullyDiscreteState^{n-k^n}(\FullyDiscreteStateArgs))
	= \bz,\qquad n=1,\ldots,\numTimeSteps,
\end{equation}
with the initial condition $\FullyDiscreteState^0(\FullyDiscreteStateArgs) =\SemiDiscreteStateIC$. In the above, $k^n$ denotes the number of steps employed by the scheme at the $n$th
time instance and 
$\residLMS^n$ denotes the FOM O$\Delta$E residual at the $n$th time instance defined as
\begin{align*}
\residLMS^n &: (\FullyDiscreteStateDummy^n;\FullyDiscreteStateDummy^{n-1},\ldots,\FullyDiscreteStateDummy^{n-k^n},\paramsDummy) \mapsto  \frac{1}{\Delta t} \sum_{j=0}^{k^n} \alpha^n_j \FullyDiscreteStateDummy^{n-j} -  \sum_{j=0}^{k^n} \beta^n_j \velocity(\FullyDiscreteStateDummy^{n-j},t^{n-j},\params),
\\
&: \RR{\FomDim} \times \RR{k^n + 1} \times \paramDomain \rightarrow \RR{\FomDim}.
\end{align*} 
Here, $\alpha^n_j,\beta^n_j\in\RR{}$, $j=0,\ldots,k^n$ are coefficients
that define the linear multistep method at the $n$th time instance.
\end{comment}

 \subsection{Trial subspaces}
Projection-based ROMs generate approximate solutions to the FOM
 by approximating the state in a low-dimensional trial
	subspace. Following the description in Ref.~\cite{parish_wls}, two types of trial subspaces are commonly used for
	this purpose:
\begin{enumerate} 
	\item \textit{Subspaces that reduce only the spatial dimension of the full-order
		model (\spatialAcronym)}. These trial subspaces are characterized by a spatial projection operator, associate with a basis that represents the spatial dependence of the state, and are employed in classic model reduction approaches, e.g., Galerkin and LSPG. %These 
	\item \textit{Subspaces that reduce both the spatial and temporal dimensions of the full-order
		model (\spaceTimeAcronym)}.
These trial subspaces are characterized by a space--time projection operator, associate with a basis that represents the spatial and temporal dependence of the state, and are employed in space--time 
model reduction approaches (e.g., space--time Galerkin~\cite{benner_st}, space--time LSPG~\cite{choi_stlspg}). 
\end{enumerate}
 We now describe these two types of space--time trial subspaces.%and their
%	application to the Galerkin, LSPG, and space--time LSPG approaches. 

\subsection{\spatialAcronym\ trial subspaces}
At a given spatial instance $\x \in \Omega$, time instance 
$t\in[0,T]$ and parameter instance $\params \in \paramDomain$,
\spatialAcronym\ trial subspaces approximate the FOM PDE solution
as $\ApproxPDEState(\PDEStateArgs) \approx \PDEState(\PDEStateArgs)$ which is enforced to reside in an affine 
trial subspace of dimension $K\ll \FomDim$ such that 
$\ApproxPDEState(\cdot,t,\params) \in \SpatialTrialSpace + \PDEStateRef(\cdot,t,\params) \subset \PDEStateSpatialTrialSpace$, where $\dim{(\SpatialTrialSpace)} = K$ and 
$\PDEStateRef(\cdot,t,\params) \in \PDEStateSpatialTrialSpace$ is a reference state, which is typically set to be constant for all time and parameters. 
%	as $\ApproxSemiDiscreteState(\SemiDiscreteStateArgs)\approx\SemiDiscreteState(\SemiDiscreteStateArgs)$, which is enforced to reside in an
%	affine spatial trial subspace of dimension $K\ll\FomDim$ such that
%	$\ApproxSemiDiscreteState(\SemiDiscreteStateArgs) \in
%	\SemiDiscreteStateRef +\SemiDiscreteSpatialTrialSpace
%\subseteq\RR{\FomDim}$, where $\dim(\SemiDiscreteSpatialTrialSpace) = K$
%and $\SemiDiscreteStateRef \in \mathbb{R}^{\FomDim}$ denotes the reference state, which
%	is often taken to be the initial condition.
The trial subspace
$\SpatialTrialSpace$ 
is spanned by a basis such that
$ \SpatialTrialSpace= \text{Span}\{\SpatialBasisVec_i\}_{i=1}^{\RomDim}$.
The basis vectors $\SpatialBasisVec_i: \x \mapsto \SpatialBasisVec_i(\x)$, $i=1,\ldots,K$ are typically constructed
using state snapshots, e.g., via
proper orthogonal decomposition (POD)~\cite{berkooz_turbulence_pod}, the reduced-basis method~\cite{rb_1,rb_2,rb_3,NgocCuong2005,Rozza2008}. 
Thus, ROMs that employ the  \spatialAcronym\
trial subspace approximate the FOM PDE solution as
\begin{equation}\label{eq:affine_trialspace}
\PDEState(\PDEStateArgs)  \approx \ApproxPDEState(\PDEStateArgs) = \sum_{i=1}^{\RomDim} \SpatialBasisVec_i(\x) \GenState_i(\SemiDiscreteGenStateArgs) + \PDEStateRef(\x,t,\params),
\end{equation}
where $\GenState(\SemiDiscreteGenStateArgs) \in \RR{\RomDim}$ denotes the generalized
coordinates. Critically, we note that the generalized coordinates in~\eqref{eq:affine_trialspace} depend on both time and parameters.

Given the trial subspace $\SpatialTrialSpace$, a reduced-order model can be formulated, e.g., via Galerkin projection, least-squares Petrov--Galerkin projection, windowed least-squares residual minimization, streamwise-upwind Petrov Galerkin projection. The result of these projection processes is a $\RomDim$-dimensional reduced-order dynamical system that must be solved for $t \in [0,T]$ for any parameter instance $\params \in \paramDomain$. These spatial-reduction ROMs have been demonstrated to yield accurate approximate solutions at a greatly reduced computational cost. As these methods only reduce the \textit{spatial} dimension of the problem, however, they can be inefficient for problems with, e.g., long time-horizons. Further, these methods can be ineffective for Kolmogorov $n$-width dominated problems, such as traveling waves.  
\begin{comment}
From the space--time perspective, this is equivalent to approximating the
	FOM ODE solution trajectory $\stateFOM\in\RR{N}\otimes\timeSpace$ with 
	$\approxstate\in \stspaceS$, where
\begin{equation}\label{eq:spatial_subspace}
\begin{split}
& \stspaceS \defeq \trialspace \otimes \timeSpace +
	\stateIntercept\otimes\onesFunction\subseteq\RR{N}\otimes\timeSpace,
\end{split}
\end{equation}
with $\onesFunction\in\timeSpace$ defined as
$\onesFunction:\timeDummy\mapsto 1$.
 
Substituting the approximation~\eqref{eq:affine_trialspace} into the FOM ODE~\eqref{eq:FOM} and performing orthogonal
$\elltwo$-projection of the initial condition onto the trial subspace yields
the overdetermined system of ODEs
\begin{equation}\label{eq:g_truncation}
\basisspace \genstateDotArg{}{t} = \velocity(\basisspace
\genstateArg{}{t} + \stateIntercept,t ), \qquad \genstate(0) = \genstateIC,
	\qquad t \in [0,T],
\end{equation}
where $\genstateDot\equiv {d \genstate}/{d\tau}$.
Because Eq.~\eqref{eq:g_truncation} is overdetermined, a solution may not
exist. Typically, either \textit{Galerkin} or \textit{least-squares
Petrov--Galerkin} projection is employed to reduce the number of equations
such that a unique solution exists. We now describe these two methods.

and the fully discrete system as
$$\DiscreteResid(\DiscreteState(\DiscreteStateArgs)) = \bz.$$ 
\end{comment}




\subsection{\spaceTimeAcronym\ trial spaces and space--time ROMs}
%Space--time projection methods that employ \spaceTimeAcronym\ trial
%spaces~\cite{choi_stlspg,constantine_strom,URBAN2012203,Yano2014ASC,benner_st,bui_thesis}
\spaceTimeAcronym\ trial
spaces aim to overcome the short-comings of \spatialAcronym\ trial subspaces by viewing the problem from a space--time perspective and reducing both the spatial and temporal dimensions of the full-order
model. Further, reduced-order models constructed from \spaceTimeAcronym\ trial subspaces yield error bounds that grow more slowly in time and
their trajectories exhibit an optimality property over the entire time domain. This section briefly outlines these subspaces.

For a given parameter instance $\params \in \paramDomain$, \spaceTimeAcronym\ trial subspaces approximate the FOM PDE solution
trajectory with an approximation that resides in an
	affine space--time trial subspace of dimension $\STDim\ll\FomDim$ such that $\PDEState(\cdot,\cdot,\params) \approx 
	\ApproxPDEState(\cdot,\cdot,\params) \in \SpaceTimeTrialSpace + \PDEStateRef(\cdot,\cdot,\params)$ with $\dim(\SpaceTimeTrialSpace) =\STDim $, where
\begin{equation}\label{eq:sttrialspace_def}
 \SpaceTimeTrialSpace \defeq 
	\text{Span}\{\SpaceTimeBasisVec_i\}_{i=1}^{\STDim} 
	\subseteq \PDEStateSpaceTimeTrialSpace.
\end{equation}
%and $\stbasis\in\RR{N \times \stdim}\otimes \timeSpace$ with 
The basis functions $\SpaceTimeBasisVec_i: (\xDummy,\timeDummy) \mapsto \SpaceTimeBasisVec_i(\xDummy,\timeDummy)$ for $i=1,\ldots,\STDim$ denotes the space--time trial basis functions, and are typically obtained via, e.g., space--time POD~\cite{}, spectral POD~\cite{}, tensor products~\cite{choi_stlspg}. Thus, ROMs that employ
\spaceTimeAcronym\ trial subspaces approximate the FOM solution as
\begin{equation}\label{eq:stapprox1}
 \PDEState(\PDEStateArgs) \approx \ApproxPDEState(\PDEStateArgs) = \sum_{i=1}^{\STDim} \SpaceTimeBasisVec_i(\x,t) \SpaceTimeGenState_i(\params) + \PDEStateRef(\PDEStateArgs),
\end{equation}
where $ \SemiDiscreteSpaceTimeGenState(\params) \in \RR{\STDim}$ denotes the space--time generalized coordinates. Comparing the approximations in Eqs.~\eqref{eq:affine_trialspace} and \eqref{eq:stapprox1} 
highlights that \spatialAcronym\ subspaces associates with time and parameter-dependent
generalized coordinates, while the \spaceTimeAcronym\ trial subspaces associates with only parameter-dependent generalized coordinates. 

Given the space--time trial subspace $\SpaceTimeTrialSpace$, a reduced-order model can be formulated via, e.g., Galerkin projection, (windowed) space--time least-squares Petrov--Galerkin projection. These projection processes result in a ROM that can be solved for any parameter instance $\params \in \ParamDomain$, and whose solution comprises the entire space--time solution for that parameter instance. Space--time ROMs are a relatively new technology, and show great promise for overcoming challenges associated with spatial-reduction only ROMs. In particular, from the space--time perspective, it is possible to capture phenomena such as traveling waves with relatively few basis vectors. However, for problems that are charcterized by, e.g., parameterized shocks, space--time ROMs can still become inaccurate due to the Kolmogorov $n$-width; for example, for steady state problems characterized by a parameterized shock location, space--time methods yield no advantage. We propose subspaces with parametric--space--time dimension reduction to address this issue.




\section{Parametric space--time dimension reduction (PSTDR) trial subspaces}
Both \spatialAcronym\ and \spaceTimeAcronym\ construct parameter-independent trial spaces. As a result, neither approach reduces the parametric dimension of the problem. Further, low-dimensional structures in an $x-t-\mu$ space may appear as high-dimensional structures in $x-t$ space, and thus both \spatialAcronym\ and \spaceTimeAcronym\ ROMs can be Kolmogorov $n$-width limited. To address these issues, we propose constructing parametric--space--time dimension reduction (\parametricSpaceTimeAcronym) subspaces. 

With \parametricSpaceTimeAcronym, we propose to approximate the parametric FOM PDE trajectory $\PDEState$, i.e., the FOM trajectory over all of space, time, and parameter instances, with an approximate parametric trajectory that lies within a low-dimensional affine parametric--space--time trial subspace of dimension $\PSTDim$, i.e., $\PDEState(\cdot,\cdot,\cdot) \approx \ApproxPDEState(\cdot,\cdot,\cdot) \in \ParametricSpaceTimeTrialSpace + \PDEStateRef(\cdot,\cdot,\cdot)$ with $\dim(\ParametricSpaceTimeTrialSpace) = \PSTDim$, where
\begin{equation}\label{eq:psttrialspace_def}
 \SemiDiscreteParametricSpaceTimeTrialSpace \defeq 
        \text{Span}\{\ParametricSpaceTimeBasisVec_i\}_{i=1}^{\PSTDim} 
        \subseteq \PDEStateParametricSpaceTimeTrialSpace.
\end{equation}
Here, $\ParametricSpaceTimeBasisVec_i: (\xDummy,\timeDummy,\paramsDummy) \mapsto \ParametricSpaceTimeBasisVec_i(\xDummy,\timeDummy,\paramsDummy)$, $i=1,\ldots,\PSTDim$ are parametric--space--time basis functions.  Thus, at any spatial location $\x \in \PhysicalDomain$, time instance $t\in[0,T]$, and parameter instance $\params \in \paramDomain$, ROMs that employ the
\parametricSpaceTimeAcronym\ trial subspace approximate the FOM PDE solution as
\begin{equation}\label{eq:pstapprox1}
 \PDEState(\PDEStateArgs) \approx \ApproxPDEState(\PDEStateArgs) = \sum_{i=1}^{\PSTDim} \ParametricSpaceTimeBasisVec_i(\PDEStateArgs) \ParametricSpaceTimeGenState + \PDEStateRef(\PDEStateArgs).
\end{equation}
Critically, comparing the approximations arising from \parametricSpaceTimeAcronym\ trial subspaces in Eq.~\eqref{eq:pstapprox1} to those arising from \spatialAcronym\ and
\spaceTimeAcronym\ trial subspaces in Eqs.~\eqref{eq:affine_trialspace} and \eqref{eq:stapprox1}, respectively,
highlights that the \parametricSpaceTimeAcronym\ approximation associates with a basis matrix that depends on space, time, and parameters. 

\section{PSTDR reduced-order models}
Given the \parametricSpaceTimeAcronym\ trial subspace, reduced-order models can be formed, e.g., via Galerkin projection, least-squares projection. Here, we detail several such approaches. In particular, we note that reduced-order models can be derived by performing a projection process on the continuous PDE~\eqref{eq:fom}, or alternatively by performing projection processes on a spatially discrete or fully discrete version of~\eqref{eq:fom}. Both approaches have advantages and disadvantages. The latter approaches have the advantages that boundary conditions are more easily incorporated into the ROM, and they retain some stability properties of the discrete full-order model. For this reason, here we develop projection-based ROMs for spatially and fully-discrete versions of~\eqref{eq:fom}; we emphasize, however, that similar approaches can be carried out at the spatially continuous level.

To the end of spatial discretization, we introduce a discretization of $\Omega$ into $\NSpace$ degrees of freedom characterized by the nodal points $\xd_i$, which in three-dimensions, for example, is given by the coordinates $\xd_i = (\xs_i,\ys_i,\zs_i)$. Spatial discrertization of the FOM PDE~\eqref{eq:fom} yields the semi-discrete system  
\begin{equation}\label{eq:fom_ode}
\frac{d \SemiDiscreteState}{dt}(\SemiDiscreteStateArgs) = \velocity(\SemiDiscreteState(\SemiDiscreteStateArgs),t,\params),
\end{equation}
where $\SemiDiscreteState(t,\params) \in \RR{\FomDim}$ is the semi-discrete state, $\FomDim = \NVars \NSpace$ is the total number of discrete degrees of freedom, and $\velocity: \RR{\FomDim} \times [0,T] \times \paramDomain \rightarrow \RR{\FomDim}$ is the velocity function.

Numerically solving the FOM ODE~\eqref{eq:fom_ode} requires temporal discretization. We consider linear multistep methods for this purpose. 
To this end, we introduce a uniform time
grid with time step $\Delta t$ and time instances
$t^n = n\Delta
t$, $n=0,\ldots,\numTimeSteps$. 
Applying a linear multistep method to discretize the FOM ODE \eqref{eq:FOM}
with this time grid
yields the FOM O$\Delta$E, which computes the sequence of discrete
solutions
$\FullyDiscreteState^n(\FullyDiscreteStateArgs) \approx \SemiDiscreteState(t^n,\params)$, $n=1,\ldots,\numTimeSteps$
as the implicit solution to the system of algebraic equations
\begin{equation}\label{eq:lms}
\residLMS^n
	(\FullyDiscreteState^n(\FullyDiscreteStateArgs);,\FullyDiscreteState^{n-1}(\FullyDiscreteStateArgs),\ldots,\FullyDiscreteState^{n-k^n}(\FullyDiscreteStateArgs))
	= \bz,\qquad n=1,\ldots,\numTimeSteps,
\end{equation}
with the initial condition $\FullyDiscreteState^0(\FullyDiscreteStateArgs) =\SemiDiscreteStateIC$. In the above, $k^n$ denotes the number of steps employed by the scheme at the $n$th
time instance and 
$\residLMS^n$ denotes the FOM O$\Delta$E residual at the $n$th time instance defined as
\begin{align*}
\residLMS^n &: (\FullyDiscreteStateDummy^n;\FullyDiscreteStateDummy^{n-1},\ldots,\FullyDiscreteStateDummy^{n-k^n},\paramsDummy) \mapsto  \frac{1}{\Delta t} \sum_{j=0}^{k^n} \alpha^n_j \FullyDiscreteStateDummy^{n-j} -  \sum_{j=0}^{k^n} \beta^n_j \velocity(\FullyDiscreteStateDummy^{n-j},t^{n-j},\paramsDummy),
\\
&: \RR{\FomDim} \times \RR{k^n + 1} \times \paramDomain \rightarrow \RR{\FomDim}.
\end{align*} 
Here, $\alpha^n_j,\beta^n_j\in\RR{}$, $j=0,\ldots,k^n$ are coefficients
that define the linear multistep method at the $n$th time instance.


%In what follow, we construct ROMs for the semi-discrete FOM ODE~\eqref{eq:fom_ode}. We emphasize, however, that similar approaches can be employed for model reduction of the continuous PDE~\eqref{eq:fom}. 

\begin{comment}
\subsection{Time-discrete reduced-order models}
We now describe time-discrete reduced-order models using linear multistep methods. 
\end{comment}
\subsubsection{Parametric--space--time Galerkin ROM}
The Galerkin ROM for \parametricSpaceTimeAcronym\ trial subspaces can be obtained by making the substitution $\SemiDiscreteState \leftarrow \ApproxSemiDiscreteState$ and by enforcing orthogonality of the residual of the FOM ODE to the trial subspace. The weak form of the problem reads as follows: find $\ApproxSemiDiscreteState \in \SemiDiscreteParametricSpaceTimeTrialSpace$ such that, $\forall t \in \TimeDomain, \params \in \paramDomain$,
\begin{equation}\label{eq:pst_galerkin}
\PSTInnerProduct{ \SemiDiscreteParametricSpaceTimeBasisVec(\SemiDiscreteStateArgs) }{ \frac{d \ApproxSemiDiscreteState}{dt}(\SemiDiscreteStateArgs) - \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) } = \bz \qquad \forall \SemiDiscreteParametricSpaceTimeBasisVec \in  \SemiDiscreteParametricSpaceTimeTrialSpace ,
\end{equation}
with $\ApproxSemiDiscreteState(0) = \mathbb{P}(\SemiDiscreteStateIC)$, where $\mathbb{P}$ is, e.g., a spatial $\ell^2$ orthogonal projection operator onto the trial subspace. In the above, $\PSTInnerProduct{\cdot}{\cdot}$ is a (weighted) parametric--space--time inner product, 
$$\PSTInnerProduct{\boldsymbol u}{ \boldsymbol v} = \int_0^T \int _{\paramDomain} \boldsymbol u^T(\SemiDiscreteStateArgs) \PSTWeightingMatrix (\SemiDiscreteStateArgs)\boldsymbol v(\SemiDiscreteStateArgs)  d \params dt, \qquad \forall \boldsymbol u, \boldsymbol v \in \SemiDiscreteParametricSpaceTimeTrialSpace$$ 
and $\PSTWeightingMatrix : \TimeDomain \times \paramDomain \rightarrow \RRSym{\FomDim\times\FomDim}$ is a symmetric weighting matrix.

Initial conditions in~\eqref{eq:pst_galerkin} can be handled in several ways, depending on the temporal discretization technique. For example, one can weakly enforce the boundary conditions by integrating by parts with respect to time to obtain the weak form: find $\ApproxSemiDiscreteState \in \SemiDiscreteParametricSpaceTimeTrialSpace$ such that, $\forall t \in \TimeDomain, \params \in \paramDomain$,
$$\left( \frac{d\SemiDiscreteParametricSpaceTimeBasisVec}{dt}(\SemiDiscreteStateArgs) ,\ApproxSemiDiscreteState(\SemiDiscreteStateArgs) \right) + \left(\SemiDiscreteParametricSpaceTimeBasisVec(\SemiDiscreteStateArgs),  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \right)  = \SemiDiscreteParametricSpaceTimeBasisVec(\cdot,\params) \ApproxSemiDiscreteState(\cdot,\params) |_{t=0}^{t=T}\qquad \forall \SemiDiscreteParametricSpaceTimeBasisVec \in  \SemiDiscreteParametricSpaceTimeTrialSpace.$$
Alternatively, the boundary conditions can be enforced by working directly with~\eqref{eq:pst_galerkin} and employing, e.g., a linear multistep method to discretize the state in time. In this approach, the initial condition can be directly enforced when computing the residual at the first time step. Critically, we note that the PST Galerkin ROM reduces to a $\PSTDim$-dimensional algebraic systems of equations.  

\subsection{Parametric--space--time least-squares ROM}
The Galerkin ROM is known to yield inaccurate results for non-symmetric and non-coercive systems. As a result, a variety of stabilized techniques have been developed, e.g., streamwise-upwind Petrov--Galerkin, least-squares Petrov--Galerkin. Here, we outline both a least-squares formulation for \parametricSpaceTimeAcronym\ trial subspaces that is continuous in time and parameter space, as well as a collocation formulation that can be solved, e.g., via a discrete least-squares solver. 

\subsubsection{Residual minimization ROMs}
In residual-minimization ROMs, we define the residual minimization principle,
\begin{equation}\label{eq:minimization_principle}
\ContinuousResMin : \SemiDiscreteState \mapsto \int_{0}^T \int_{\paramDomain} \| \frac{d \SemiDiscreteState}{dt} - \velocity(\SemiDiscreteState,t,\params) \|_{\PSTWeightingMatrix(t,\params)}^p d\params dt,
\end{equation}
for $p>0$ (typically $p=1$ or $p=2$).
An approximate solution is defined as one that minimizes this principle, 
\begin{equation}\label{eq:minimization_problem}
\ApproxSemiDiscreteState = \underset{ \SemiDiscreteStateDum \in \SemiDiscreteParametricSpaceTimeTrialSpace}{\text{arg min}}\;\ContinuousResMin(\SemiDiscreteStateDum),
\end{equation}
subject to $\ApproxSemiDiscreteState(\params) = \mathbb{P}(\SemiDiscreteStateIC).$ 

Various numerical techniques capable of solving the minimization problem~\eqref{eq:minimization_principle} exist. For example, one can take an \textit{indirect}, or \textit{optimize-then-discretize} approach, where weak forms associated with the minimization principle~\eqref{eq:minimization_principle} are obtained, e.g., via the Euler--Lagrange equations (see Ref.~\cite{parish_wls} for the case where $p=2$). In recent years, \textit{direct}, or \textit{discretize-then-optimize} methods have become popular. In these discretize-then-optimize methods, the continuous residual minimization principle~\eqref{eq:minimization_principle} is replaced with a discrete minimization principle, and the resulting system is solved via an algebraic least-squares solver. Due to its ease of implementation and robustness, we outline this approach. 

Direct approaches require temporal and parametric discretization of the FOM ODE~\eqref{eq:fom_ode}. Here we employ linear multistep methods for discretization of the temporal derivatives, and quadrature rues for discretization of the integrals. For simplicity of presentation, we introduce a uniform time
grid with time step $\Delta t$ and time instances
$t^n = n\Delta
t$, $n=0,\ldots,\numTimeSteps$. 
Applying a linear multistep method to discretize the FOM ODE \eqref{eq:FOM}
with this time grid
yields the FOM O$\Delta$E, which computes the sequence of discrete
solutions
$\FullyDiscreteState^n(\FullyDiscreteStateArgs) \approx \SemiDiscreteState(t^n,\params)$, $n=1,\ldots,\numTimeSteps$
as the implicit solution to the system of algebraic equations
\begin{equation}\label{eq:lms}
\residLMS^n
	(\FullyDiscreteState^n(\FullyDiscreteStateArgs),\FullyDiscreteState^{n-1}(\FullyDiscreteStateArgs),\ldots,\FullyDiscreteState^{n-k^n}(\FullyDiscreteStateArgs))
	= \bz,\qquad n=1,\ldots,\numTimeSteps,
\end{equation}
with the initial condition $\FullyDiscreteState^0(\FullyDiscreteStateArgs) =\SemiDiscreteStateIC$. In the above, $k^n$ denotes the number of steps employed by the scheme at the $n$th
time instance and 
$\residLMS^n$ denotes the FOM O$\Delta$E residual at the $n$th time instance defined as
\begin{align*}
\residLMS^n &: (\FullyDiscreteStateDummy^n,\FullyDiscreteStateDummy^{n-1},\ldots,\FullyDiscreteStateDummy^{n-k^n},\paramsDummy) \mapsto  \frac{1}{\Delta t} \sum_{j=0}^{k^n} \alpha^n_j \FullyDiscreteStateDummy^{n-j} -  \sum_{j=0}^{k^n} \beta^n_j \velocity(\FullyDiscreteStateDummy^{n-j},t^{n-j},\params),
\\
&: \RR{\FomDim} \times \RR{k^n + 1} \times \paramDomain \rightarrow \RR{\FomDim}.
\end{align*} 
Here, $\alpha^n_j,\beta^n_j\in\RR{}$, $j=0,\ldots,k^n$ are coefficients
that define the linear multistep method at the $n$th time instance.
 
Next, we introduce a discretization of the parametric domain $\mathcal{D}$ into $N_{\numQuadPointsParams}$ nodal points with locations $\{ \DiscreteParams^i \}_{i = 1}^{\numQuadPointsParams}$, where $\DiscreteParams^i = (\DiscreteParams^i_1 , \ldots,\DiscreteParams^i_{\numParams}) \in \paramDomain$, $i=1,\ldots,\numQuadPointsParams$. The nodal points can be determined, e.g., via uniform Cartesian discretization, Gaussian quadrature, Smolyak quadrature. Discretization of the temporal and parametric domain enables us to replace the continuous minimization principle~\eqref{eq:minimization_principle} with the discrete minimization principle
 \begin{equation}\label{eq:discrete_minimization_principle}
\DiscreteResMin : \ParametricFullyDiscreteStateDummy  \mapsto \sum_{n=1}^{\numTimeSteps}  \sum_{j=1}^{\numQuadPointsParams} \QuadWeights_{ij} \| \residLMS^n (\FullyDiscreteStateDummy^{n,j},\FullyDiscreteStateDummy^{n-1,j},\ldots,\FullyDiscreteStateDummy^{n-k^n,j},\DiscreteParams^j) \|_{\PSTWeightingMatrix(t^n,\params^j)}^p,
\end{equation}
where $\ParametricFullyDiscreteStateDummy^{nj} \in \RR{\FomDim \numTimeSteps \numQuadPointsParams} =  \begin{bmatrix} \FullyDiscreteStateDummy^{1,1} & \cdots & \FullyDiscreteStateDummy^{\numTimeSteps,1} & \FullyDiscreteStateDummy^{1,2} & \cdots & \FullyDiscreteStateDummy^{\numTimeSteps,\numQuadPointsParams} \end{bmatrix}^T \in \RR{\FomDim \numTimeSteps \numQuadPointsParams}$, with $\FullyDiscreteState^{i,j} \in \RR{\FomDim}$ and $\QuadWeights^{nj} \in \RR{}$, $i=1\dots,\numTimeSteps$, $j=1,\ldots,\numQuadPointsParams$ are quadrature weights. 

An approximate solution obtained from the discreate residual minimization formulation is given as
\begin{equation}\label{eq:discrete_minimization_principle_solution}
\ParametricSpaceTimeGenState = \underset{\ParametricSpaceTimeGenStateDum \in \RR{\PSTDim} }{\text{arg min} } \; \DiscreteResMin( \DiscreteParametricSpaceTimeBasisMat \ParametricSpaceTimeGenStateDum),
\end{equation}
where $\DiscreteParametricSpaceTimeBasisMat \equiv \begin{bmatrix} \left[ \ParametricSpaceTimeBasisMat(\xd,t^1,\DiscreteParams^1)\right]^T & \cdots & \left[\ParametricSpaceTimeBasisMat(\xd,t^{\numTimeSteps},\DiscreteParams^1) \right]^T & \left[ \ParametricSpaceTimeBasisMat(\xd,t^1,\DiscreteParams^2) \right]^T & \cdots & \left[\ParametricSpaceTimeBasisMat(\xd,t^{\numTimeSteps},\DiscreteParams^{\numQuadPointsParams}) \right]^T \end{bmatrix}^T \in \RR{\FomDim \numTimeSteps \numQuadPointsParams \times \PSTDim}$ is the parametric--space--time basis evaluated on the parametric--space--time grid.

%The weak form given from this residual minimization principle is given as follows: find $\ApproxSemiDiscreteState \in \SemiDiscreteParametricSpaceTimeTrialSpace$ such that, $\forall t \in \TimeDomain, \params \in \paramDomain$,
%  rather than working directly with the FOM ODE~\eqref{eq:fom}, we instead work with the normal equations,
%\begin{equation}\label{eq:FOM_normal}
% \bigg[\big[\frac{\partial \velocity}{\partial \boldsymbol y}(  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  \big]^T  + \frac{d}{dt} \bigg] \bigg( \frac{d}{dt}\ApproxSemiDiscreteState(\SemiDiscreteStateArgs) -   \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \bigg) = \bz
%\end{equation} 
%with an initial boundary condition $\SemiDiscreteState(0,\params) = \SemiDiscreteStateIC$ and a terminal boundary condition $\SemiDiscreteState(T,\params) -   \velocity(\ApproxSemiDiscreteState(T,\params),T,\params) = \bz$. The normal equations have the advantage that they are symmetric, which can improve the stability properties of a ROM.

%\begin{comment}
% \begin{equation}\label{eq:ROM_normal1}
%\PSTInnerProduct{ \SemiDiscreteParametricSpaceTimeBasisVec}{ \bigg[\big[\frac{\partial \velocity}{\partial \boldsymbol y}(  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  \big]^T  + \frac{d}{dt} \bigg] \bigg( \frac{d}{dt}\ApproxSemiDiscreteState(\SemiDiscreteStateArgs) -   \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \bigg)} = \bz
%\end{equation} 
%
% \begin{equation}\label{eq:ROM_normal1}
%\PSTInnerProduct{ \SemiDiscreteParametricSpaceTimeBasisVec}{ \big[\frac{\partial \velocity}{\partial \boldsymbol y}(  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  \big]^T  \left( \frac{d}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs)  -   \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \right) + \frac{d^2}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs) - \frac{d}{dt}  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  } 
% = \bz
%\end{equation} 
% \begin{multline}\label{eq:ROM_normal2}
%\PSTInnerProduct{ \SemiDiscreteParametricSpaceTimeBasisVec}{ \big[\frac{\partial \velocity}{\partial \boldsymbol y}(  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  \big]^T  \left( \frac{d}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs)  -   \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \right)} 
%- \PSTInnerProduct{\frac{d \SemiDiscreteParametricSpaceTimeBasisVec}{dt}}{ \frac{d}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs) -  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  } 
% =  \\
%-  \SemiDiscreteParametricSpaceTimeBasisVec \left( \frac{d}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs)   - \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \right)_0^T
%\end{multline} 
%\end{comment}
% \begin{multline}\label{eq:ROM_normal2}
%\PSTInnerProduct{ \SemiDiscreteParametricSpaceTimeBasisVec}{ \big[\frac{\partial \velocity}{\partial \boldsymbol y}(  \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  \big]^T  \left( \frac{d}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs)  -   \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \right)} 
%+ \PSTInnerProduct{\frac{d \SemiDiscreteParametricSpaceTimeBasisVec}{dt}}{   \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params)  } \\
%- \PSTInnerProduct{\frac{d^2 \SemiDiscreteParametricSpaceTimeBasisVec}{dt^2}}{  \ApproxSemiDiscreteState(\SemiDiscreteStateArgs)} 
% =  
%-  \SemiDiscreteParametricSpaceTimeBasisVec \left( \frac{d}{dt} \ApproxSemiDiscreteState(\SemiDiscreteStateArgs)   - \velocity(\ApproxSemiDiscreteState(\SemiDiscreteStateArgs),t,\params) \right)_0^T - \frac{d \SemiDiscreteParametricSpaceTimeBasisVec}{dt}\ApproxSemiDiscreteState|_0^T \qquad \forall \SemiDiscreteParametricSpaceTimeBasisVec \in \SemiDiscreteParametricSpaceTimeTrialSpace
%\end{multline} 
%
\subsection{Hyper-reduction}
Both the Galerkin ROM~\eqref{eq:pst_galerkin} and the residual minimization ROM~\eqref{eq:discrete_minimization_principle_solution} require solving for $\PSTDim \ll \FomDim \numTimeSteps \numQuadPointsParams$ generalized coordinates. However, for general nonlinear problems, algorithms to solve these problems scale with the full dimension of the parametric--space--time grid; this is because the quadrature rules in Eqs.~\eqref{eq:pst_galerkin} and~\eqref{eq:discrete_minimization_principle_solution} need to be evaluated each time the solution is updated. \textit{Hyper-reduction} techniques, such as the (discrete) empirical interpolation method, have been developed to circumvent this bottleneck. Here, we outline several hyper-reduction approaches. 
\begin{comment}
\subsubsection{Collocated minimum-residual ROMs}
Collocation-based residual-minimization ROMs work with a discrete minimization principle, as opposed to the continuous one described in~\eqref{eq:}. Specifically, we discretize the parametric--space--time domain to obtain the set of collocation points, 
$$\CollocationSet = \{ \x_i,t_i,\mu_i\}_{i=1}^{\NCollocation},$$
where $\NCollocation \ge \PSTDim$ denotes the number of collocation points. These collocation points can be obtained via, e.g., tensor grids, sparse tensor grids, random sampling. With these collocation points, we can define the residual minimization principle 
$$\mathbf{J} = \sum_{i=1}^{\NCollocation} \ResidualLoss\left( \dot{\SemiDiscreteParametricSpaceTimeBasisMat}(\x_i,t_i,\params_i)\SemiDiscreteParametricSpaceTimeGenState - \velocity( \SemiDiscreteParametricSpaceTimeBasisMat(\x_i,t_i,\params_i) \SemiDiscreteParametricSpaceTimeGenState ,t_i,\params_i) \right),$$
where $\ResidualLoss : \RR{} \rightarrow \RR{+}$ is, e.g., an $\ell^2$ loss function. 
\end{comment}
\section{Construction of \parametricSpaceTimeAcronym\ trial subspaces via deep learning}
We now outline a mesh-free deep-learning-based strategy for constructing of the \parametricSpaceTimeAcronym\ trial subspaces. We note that other approaches exist for computing these subspaces, e.g., tensor-product bases~\cite{choi_stlspg}, piecewise-constant subspaces via local bases~\cite{}, vector-basis splitting on the space--time--parametric solution snapshot~\cite{carlberg_hadaptation,ETTER2020112931}. Here, we pursue a deep-learning-based approach due to the expressiveness and flexibility of the resulting subspace.
\subsection{Training data}
As is customary in the traditional offline-online paradigm, we assume access to an \textit{a priori} set of \textit{training data} comprising solution snapshots of the full-order model at various spatial, temporal, and parametric instances,
$$\TrainingData = \{\PDEState(\x_i,t_i,\params_i) \}_{i=1}^{\NTrain}.$$ 
In the offline stage, this training data is used to identify a low-dimensional subspace capable of representing the majority of the variance of the training data. Typically, this is achieved, e.g., via POD, the reduced basis method. 
In the present context, unfortunately, these methods cannot be directly applied as, when viewed from the parametric--space--time perspective, the training data comprises only a single training instance, and thus only a single basis vector can be extracted. While this basis vector yields perfect reconstruction error on the training data, it may be inaccurate for prediction at new data points. Potential approaches to address this issue could include tensor-product bases, h-adaptation, and parametric-piecewise-constant subspaces via local bases. 

Here we propose a deep-learning-based approach to obtain the low-dimensional space--time--parametric trial subspace. Specifically, we take advantage of the fact that feed-forward neural networks with a linear final layer can be understood as approximation techniques that learn a data-driven trial subspace in training. We will refer to these associated trial subspaces as \textit{deep subspaces}, and now describe their construction. 
\subsection{Deep subspaces}
In the proposed \MLSubspaceNameLowercase, we aim to construct data-driven subspaces --- that depend on space, time, and parameters --- that are spanned by a set of basis vectors, 
$$\ParametricSpaceTimeTrialSpace \equiv \text{span}\{\ParametricSpaceTimeBasisVec_i\}_{i=1}^{\PSTDim}.$$
Here, we propose to describe these basis vectors via deep feedforward neural networks,
$$\ParametricSpaceTimeBasisMat (\PDEStateArgs)\equiv \begin{bmatrix} \ParametricSpaceTimeBasisVec_1(\PDEStateArgs) &
\hdots & 
 \ParametricSpaceTimeBasisVec_{\PSTDim}(\PDEStateArgs) \end{bmatrix}^T
= \NeuralNetwork(\PDEStateArgs;\NNWeights).
$$
\begin{figure}
\begin{center}
\input{tikz_nn}
\caption{Depiction of a deep neural network for the bases, $\SemiDiscreteParametricSpaceTimeBasisMat$.}
\end{center}
\end{figure}
where $\NeuralNetwork$ is a neural network defined by
\begin{align}
&\NeuralNetwork : (\xDummy,\timeDummy,\paramsDummy;\NNWeights) \mapsto \text{reshape}(\cdot,\PSTDim ,\NVars) \circ \activation_d(\cdot,\NNWeights_d) \circ \cdots \circ \activation_1(\xDummy,\timeDummy,\paramsDummy; \NNWeights_1) \\
&\NeuralNetwork(\cdot,\cdot,\cdot;\NNWeights) : \PhysicalDomain \times \TimeDomain \times \paramDomain \rightarrow \RR{\PSTDim \times \NVars}.
\end{align}
In the above, $\NNWeights \equiv (\NNWeights_1,\cdots,\NNWeights_d) \in \RR{\NumWeights}$ with $\NNWeights_i \equiv (\Weights_i,\biases_i) \in \RR{p_i \times p_{i-1}} \times \RR{p_i}$, $i=1,\ldots,d$ are the weights and biases, $\activation_i : \RR{p_{i-1}} \rightarrow \RR{p_i}$, $i=1,\ldots,d$ denotes the activation function employed at the $i$th layer, $d$ denotes the depth of the network, and $p_i, i=0,\ldots,d$ (with $p_0 = \numParams + \numSpaceDims + 1$ and $p_d = \NVars \PSTDim$) denotes the number of neurons at layer $i$. The activation functions take the form
$$\activation_i: (\boldsymbol w, \NNWeights_i) \mapsto \activationFunc_i(\Weights_i \boldsymbol w + \biases_i), \; \; i=1,\ldots,d,$$
where $\activationFunc_i : \RR{p_{i-1}} \rightarrow \RR{p_i}$ denote activation functions (e.g., exponential linear units) that are applied elementwise.  

Before proceeding, we make two critical remarks regarding these deep subspaces. 

\begin{remark}               
\textbf{Increasing subspace dimension does not yield lower projection errors for training data.} In the traditional ROM paradigm, more accurate trial subspaces are obtained from retaining additional basis functions. In the present context, the dimension of the subspace can be controlled setting the number of basis vectors in the final layer of the network. Growing the number of basis functions does not necessarily translate to increased accuracy, however. The explanation for this is as follows: as the proposed framework constructs ROMs from the parametric--space--time viewpoint, the training data can be described by a \textbf{single} basis vector, i.e., this single basis vector is the training data solution. Thus, there exists a one-dimensional trial subspace that describes the data. In practice, learning a network that is capable of describing this solution vector with a single basis vector can be challenging, and employing multiple basis vectors may have practical advantages.  
\end{remark}
\begin{remark}               
\textbf{Increasing subspace dimension can yield lower projection errors for testing data.} While increasing the subspace dimension does not yield improved projection errors on the training data, it can yield lower projection errors for testing data. The reason for this is that the single basis function described by the training data solution will not yield zero projection errors for testing data, and as such is can be advantageous to have multiple basis functions for the predictive setting of the ROM. 
\end{remark}


\subsection{Training}
To train the weights $\NNWeights$, we aim to learn a subspace that is optimal in reconstructing the training data (e.g., in the least-squares sense). To this end, we define the loss function 
$$\mathcal{L}(\NNWeights,\ParametricSpaceTimeGenStateNN) = \sum_{i=1}^{\NTrain} \| \PDEState(\x_i,t_i,\params_i) - \left[\NeuralNetwork(\x_i,t_i,\params_i;\weights)\right]^T \ParametricSpaceTimeGenStateNN \|_2^2$$  
where $\SemiDiscreteParametricSpaceTimeGenStateNN \in \RR{\PSTDim}$ act as generalized coordinates. We then solve the (offline) minimization problem,
\begin{equation}\label{eq:offline_min}
\NNWeights^*,\SemiDiscreteParametricSpaceTimeGenStateNN^* = \underset{ \NNWeights \in \RR{\NWeights},\SemiDiscreteParametricSpaceTimeGenStateNN \in \RR{\PSTDim}}{\text{arg min}} \mathcal{L}(\NNWeights,\SemiDiscreteParametricSpaceTimeGenStateNN).
\end{equation} 
The solution to this minimization problem yields the optimal weights $\NNWeights^*$, as well as a set of generalized coordinates that minimize the error on the training data (in the sense defined in the objective function, e.g., least-squares, $\ell^1$). Various approaches can be employed to solve the optimization problem~\eqref{eq:offline_min}, including stochastic gradient decent, Adam, and alternating least-squares.

\subsection{Deep ensemble subspaces} 
Almost all modern algorithms for training neural networks are stochastic. As the optimization problem~\eqref{offline_min} does not, in general, have a unique solution, it is expected that a different set of bases functions will be obtained each time the network is trained. This stochastic training process can have benefits; for example, the use of ensemble models is one way of quantifying extrapolation that has garnered recent attention in deep learning~\cite{deep_ensembles}. Here, we propose two ways that stochastic training can be leveraged:
\begin{enumerate}
\item \textbf{Ensemble ROMs for empirical error detection:} It has been empirically observed that independently trained deep neural networks yield similar results when queried for \textit{in-distribution} data. When queried for \textit{out-of-distribution} data, however, results are known to vary significantly. From this observation, ensemble-based approaches have garnered attention as methods to assess the accuracy of an ML model on novel, potentially out-of-distribution, training data~\cite{deep_ensembles}. Here, we propose that stochastic training can be used to develop an ensemble of ROMs, where each ROM uses its own independently learned subspace. The variance of the ROM solutions can then be used to assess accuracy.   

\item \textbf{Subspace enrichment:} We can leverage the linearity of our trial subspaces, and the stochastic training process, to perform subspace enrichment. In this approach, we again train an ensemble of subspace models in the offline phase. As opposed to treating each subspace individually in the online phase, however, we can combine them to construct a single enriched subspace.
\end{enumerate}  

\section{Computational considerations}

\begin{algorithm}[H]
\caption{Algorithm for an implicit Euler update for the LSPG ROM using a Gauss-Newton method with Gaussian Elimination}
\label{alg:alg_LSPG}
%\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
Input: $\FullyDiscreteState^n$, residual tolerance $\epsilon$ \;
Output: $\FullyDiscreteState^{n+1}$\;
Steps:
\begin{enumerate}
\item Set initial guess, $\FullyDiscreteState_k$
\item  Loop while $\mathbf{r}_k > \epsilon$
\begin{enumerate}
    \item Compute the state from the generalized coordinates, $\ApproxFullyDiscreteState  =\ParametricSpaceTimeBasisMat \ParametricSpaceTimeGenState_{k}$
    \item Compute the residual from the full state, $\residLMS(\ApproxFullyDiscreteState)$
    \item Compute the Jacobian, $\mathbf{J}_k = \frac{\partial \residLMS}{\partial \ParametricSpaceTimeGenState} \ParametricSpaceTimeBasisMat $
    \item Compute the product, $\mathbf{J}_k^T \mathbf{J}_k$
    \item Project the residual onto the test space, $\mathbf{J}^T \residLMS$
    \item Solve $\FullyDiscreteJacobian^T \FullyDiscreteJacobian \Delta \ParametricSpaceTimeGenState = - \FullyDiscreteJacobian^T \residLMS$ via Gaussian elimination
    \item Update solution, $\ParametricSpaceTimeGenState_{k+1} = \ParametricSpaceTimeGenState_k + \Delta \ParametricSpaceTimeGenState$
    \item k = k + 1
\end{enumerate}
\item Set final state, $\ParametricSpaceTimeGenState^{n+1} = \ParametricSpaceTimeGenState_k$
\end{enumerate}
\end{algorithm}




\section{Numerical experiments}
We now present numerical experiments of the proposed framework.

\subsection{Burgers' Equation}
We first consider the benchmark problem of the parameterized Burgers' equation. We consider a problem setup similar to~\ref{LeeCarlberg}, but with less training data. The problem setup is given as follows: 
\begin{align*}\label{eq:burgers_pde}
&\frac{\partial \PDEState}{\partial t}(x,t,\params) + \frac{1}{2} \frac{\partial \PDEState^2}{\partial x}(x,t,\params) = \frac{1}{50} \exp(\params_2 x). \\
&u(x,0,\params) = 1, \; u(1,0,\params) = \params_1,
\end{align*}
for $x \in [0,100]$, $t \in [0,30]$, with $\params_1 \sim U[4,6]$, $\params_2 \sim U[0.01,0.04]$.
The problem is characterized by a parameterized discontinuity moving left to the right through the domain. Our semi-discrete discretization of~\eqref{eq:burgers_pde} comprises a first-order finite volume discretization with $\NSpace=256$ degrees of freedom. The first-order upwind flux is used at the cell interfaces, and the second-order Crank-Nicolson scheme with a time step of $\Delta t = 0.07$ is employed for time stepping.

\subsubsection{Training and testing data}
We now describe the training and testing data used in the following experiments. For training data, we execute solves of the full-order model for the Cartesian parameter grid $\mu_1 \times \mu_2 = \{ 4.25 + 0.3125 i \}_{i=0}^4 \times \{ 0.015 + 0.05 j \}_{j=0}^3$. We collect snapshots of the FOM every $10$ time steps for $t \in [0,15]$, resulting in a total of 50 temporal snapshots. In total the dataset comprises $256 \times 5 \times 4 \times 50 = 256,000$ data points.

We consider two datasets for testing: one dataset that \textit{interpolates} between the training data, and one dataset that involves \textit{extrapolating} beyond the training data. For this first dataset, we execute solves of the full-order model for the Cartesian parameter grid $\mu_1 \times \mu_2 = \{4.40625 + 0.3125 i \}_{i=0}^3 \times \{ 0.0175 + 0.05 j \}_{j=0}^2$ with a time step of $\Delta t = 0.07$ for $t \in [0,15]$. For the second dataset, we execute solves of the full-order model for the Cartesian parameter grid $\mu_1 \times \mu_2 = \{4.0 + \frac{2}{7} i \}_{i=0}^7 \times \{ 0.01 + \frac{0.03 j}{7} \}_{j=0}^7$ with a time step of $\Delta t = 0.07$ for $t \in [0,30]$. 

%To construct our bases, we nominally employ a deep MLP characterized by 6 layers with widths $\{l_1,\cdots,\l_6\} = \{8,16,32,64,128,8\}$, thus the total number of bases vectors employed is $\PSTDim = 8$. A parametric study of different networks is provided later. For training, we employ the Adam algorithm to minimize the MSE over the training data. We employ a learning rate schedule of $\text{lr} = \{5e-4,2e-4,1e-4,5e-5,2e-5,1e-5 \}$, where we switch rates after $1000,2000,5000,10000,$ and $15000$ epochs, respectively. 

%We assess two different ROM formulations in what follows. First, we present results for an ensemble of 5 ROMs, where each ROM employs a separate trial subspace obtained by the training procedure presented above. We present ensemble results to (1) fairly assess the methodology as training is stochastic and (2) demonstrate how ensembles can be employed for empirical error detection, as discussed above. In the second formulation, we present ROMs obtained with subspace enrichment. In this process, we train one trial subspace via the procedure presented above, and then train an additional 10 trial subspaces via for 1000 epochs with a learning rate schedule of $\text{lr} = \{5e-3,1e-3,5e-4,1e-4\}$ with switch points after 500, 750, and 900 epochs, respectively.  

\subsection{Results: A priori projection errors, convergence with basis dimension, and the generalizability gap}
We first assess the impact of the network complexity and dimension of the trial space on the impact of training and generalizability. To this end, we train a total of 150 deep neural networks with the architecture described in ..., with $\text{depth} = \{1,2,3,4,5\}$, and $\PSTDim = \{5,10,15,20,25,30,35,45,50\}$. For each configuration, we train the network three times so-as to account for stochastic training effects. We train each network for 10,000 epochs with learning rates of $5e-3,1e-4,5e-4,2e-4,5e-5$ with switching points of $1000,3000,7000,9000$. We assess the following metrics:
\begin{enumerate}
\item Mean-squared-error for on the training set: This metric indicates how well our trial space is capable of representing the training data. We expect that, as the expressiveness of the network increases, this error can be driven to zero. Formally, this metric is defined as
$$\text{MSE} = \frac{1}{ \| N_{train} \| } \sum_{i=1}^{N_{train}} \| \ApproxSemiDiscreteState(t_i,\params_i) - \SemiDiscreteState(t_i,\params_i) \|_2.$$

\item Mean-squared-error between the test data and their $\ell^2$ projection onto the trial subspace: This metric indicates how well our trial subspace can generalize to new data, and provides an upper bound on how accurate the ROM can be.

\item Relative error between optimal projection of the test data, and the ML prediction: Constructing the deep subspaces involves optimizing for a set of generalized coordinates, and this model can be directly used to make predictions at new data points. This metric provides an indicator on how much a ROM can improve upon the ML prediction.

\item The generalizability gap: This final metric is defined as the relative error between the optimal projection of the training set and that of the testing set. This metric provides an indicator for how well the deep subspaces can represent the test data, with respect to how well they represented the training data.  
\end{enumerate}

Figure~\ref{fig:burg_training_test_projection_mse} depicts the ML-ROM error and the $\ell^2$-orthogonal projection error for the training and testing datasets. We observe that both methods yield similar results that associate with low MSEs for the training set, implying that the ML ROM predicts generalized coordinates that nearly minimize the mean-squared error. We observe that increasing the network depth tends to lower MSE, but increasing the number of basis functions does not tend to decrease the MSE; see Remark~\cite{}. Next, we observe both methods to associate with higher MSEs for the testing dataset. Importantly, we observe that the error associated with $\ell^2$-orthogonal projection onto the trial subspace is lower than the error associated with the ML ROM. We also note that increasing the depth of the network tends to slightly decrease the MSE in the case of $\ell^2$-orthogonal projection, but not as significantly as observed for the training data; we do not observe that increasing the depth of the network improves ML-ROM error. Next, we observe that increasing the basis dimension yields lower MSE in the case of $\ell^2$ orthogonal projection, see Remark~\cite{}. We do not observe that growing the basis dimension yields lower MSEs for the ML-ROM.    
\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/MSE_training.pdf}
\caption{$\ell^2$-orthogonal projection (training data)}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/MSE_training.pdf}
\caption{ML-ROM (training data)}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/MSE_testing.pdf}
\caption{$\ell^2$-orthogonal projection (testing data)}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/MSE_testing_ML.pdf}
\caption{ML-ROM (testing data)}
\end{subfigure}
\caption{Mean-squared-errors associated with $\ell^2$-orthogonal projection (left) and the ML-ROM (right) for the training data (top) and testing data (bottom).}
\label{fig:burg_training_test_projection_mse}
\end{center}
\end{figure}

Figure~\ref{fig:burg_gen_gap} depicts the generalization gap associated with $\ell^2$-orthogonal projection and the ML-ROM. We observe that, in general, the generalization error grows as the depth is increased for both $\ell^2$-orthogonal projection and the ML-ROM. Growing the basis dimension has a small impact on the generalization error. We additionally observe that the generalization error associated with $\ell^2$-orthogonal projection is lower than the ML-ROM. 
\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/gen_gap.pdf}
\caption{$\ell^2$-orthogonal projection}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/gen_gap_ML.pdf}
\caption{ML-ROM}
\end{subfigure}
\caption{Generalization error for $\ell^2$ orthogonal projection (left) and the ML-ROM (right).}
\label{fig:burg_gen_gap}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/rel_improvement.pdf}
\caption{Least-squares ROM relative error}
\end{subfigure}
\label{fig:burg_training_results}
\end{center}
\end{figure}



\subsection{Results: Extrapolation in parameter space and time}
We now present results for the framework with both extrapolation in parameter space and in time. We execute reduced-order models on the $8x8$ Cartesian parameter grid  $\mu_1 \times \mu_2 = \{ 4. 0+  \frac{2i}{7} \}_{i=0}^7 \times \{ 0.01 + \frac{j}{70} \}_{j=0}^7$ for $t \in [0,30]$, which comprises extrapolation in both the parameter and temporal dimension. Table~\ref{tab:burg_results} tabulates the mean error and standard deviation for the various ROM methods, where the error and standard deviation are computed from the 5 MLPs. Similarly, Figure~\ref{fig:burg_phys_space} depicts ensembles of physical space solutions for the various ROMs. We observe that all methods are accurate, particularly in the interpolation regime. We observe that, on average, the $\ell^1$ res-min ROM yields the most accurate solutions, followed by the ML and least-squares ROMs. The Galerkin method was only stable for 3 of 5 runs; when stable, Galerkin yields reasonable results, but not as accurate as the other approaches.


\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/MSE_LS.pdf}
\caption{PST-LSPG}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/MSE_L1.pdf}
\caption{PST-LSPG ($\ell^1$)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/MSE_ML.pdf}
\caption{ML-ROM}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/L1_LS.pdf}
\caption{PST-LSPG}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/L1_L1.pdf}
\caption{PST-LSPG ($\ell^1$)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/L1_ML.pdf}
\caption{ML-ROM}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/Linf_LS.pdf}
\caption{PST-LSPG}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/Linf_L1.pdf}
\caption{PST-LSPG ($\ell^1$)}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/Linf_ML.pdf}
\caption{ML-ROM}
\end{subfigure}

\caption{Mean-squared-errors associated with $\ell^2$-orthogonal projection (left) and the ML-ROM (right) for the training data (top) and testing data (bottom).}
\label{fig:burg_training_test_projection_mse}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/LS_hist_MSE.pdf}
\caption{LS}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/L1_hist_MSE.pdf}
\caption{L1}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/ML_hist_MSE.pdf}
\caption{ML}
\end{subfigure}
%\begin{subfigure}[t]{0.32\textwidth}
%\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/rel_MSE_L1.pdf}
%\caption{PST-LSPG ($\ell^1$)}
%\end{subfigure}
%\begin{subfigure}[t]{0.32\textwidth}
%\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/rel_L1_L1.pdf}
%\caption{PST-LSPG ($\ell^1$)}
%\end{subfigure}
%\begin{subfigure}[t]{0.32\textwidth}
%\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/basis_study/results/rel_Linf_L1.pdf}
%\caption{PST-LSPG ($\ell^1$)}
%\end{subfigure}
\caption{Mean-squared-errors associated with $\ell^2$-orthogonal projection (left) and the ML-ROM (right) for the training data (top) and testing data (bottom).}
\label{fig:burg_training_test_projection_mse}
\end{center}
\end{figure}





Next, Figure~\ref{fig:error_vs_params} presents relative errors averaged over the ensemble for the various ROMs. We observe that all ROMs yield similar results: Sub 1\% relative errors are obtained for all parameters within the training regime, and sub 1.5\% relative errors are obtained for extrapolation cases. We additionally observe that the models extrapolate much better in the $\mu_2$ parameter than in the $\mu_1$ parameter. 
\begin{table}[]
\begin{centering}
\begin{tabular}{l l l l}
\hline
  & $\ell^2$-error  & Residual $\ell^2$-norm & $\ell^2$-error (best) \\
\hline
ML-ROM    & $788.014 \pm 102.694 $ & $35.640 \pm 3.514$  &  $652.778 $ \\
Least-squares ROM & $822.000 \pm 108.855$ & $34.040 \pm 3.004$ & $661.549$\\
$\ell^1$ ROM    & $776.741 \pm 105.274$ &  $35.103 \pm  3.189$ & $652.880$\\
Galerkin ROM    & NA &  NA & $689.189$  \\
\hline
\end{tabular}
\caption{Summary of the various basis sizes employed in the cavity flow example.}
\label{tab:burg_results}
\end{centering}
\end{table}

\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usol_0001.pdf} 
\caption{$t=7.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usol_0003.pdf} 
\caption{$t=10.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usol_0005.pdf} 
\caption{$t=21.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usol_0007.pdf} 
\caption{$t=28.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate_0001.pdf} 
\caption{$t=7.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate_0003.pdf} 
\caption{$t=10.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate_0005.pdf} 
\caption{$t=21.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate_0007.pdf} 
\caption{$t=28.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate2_0001.pdf} 
\caption{$t=7.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate2_0003.pdf} 
\caption{$t=10.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate2_0005.pdf} 
\caption{$t=21.0$}
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/usolExtrapolate2_0007.pdf} 
\caption{$t=28.0$}
\end{subfigure}

\caption{Physical solutions for ROMs of the 1D Burgers equation at various time instances. Parameter interpolation (top), and parameter extrapolation (bottom).}
\label{fig:burg_phys_space}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/uls_error_vs_param.pdf} 
\caption{Least-squares ROM relative error}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/ul1_error_vs_param.pdf} 
\caption{$\ell^1$ ROM relative error}
\end{subfigure}
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=1.0\linewidth]{code/burgers/synapse_models/elu/results/uml_error_vs_param.pdf} 
\caption{ML ROM relative error}
\end{subfigure}
\label{fig:rom_metrics_swe_updatefreq}
\end{center}
\end{figure}

\bibliographystyle{siam}
\bibliography{refs}

\end{document}

